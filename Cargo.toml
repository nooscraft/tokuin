[package]
name = "tokuin"
version = "0.2.0"
edition = "2021"
authors = ["Tokuin Contributors"]
license = "MIT OR Apache-2.0"
description = "A fast CLI tool to estimate token usage and API costs for LLM prompts"
repository = "https://github.com/nooscraft/tokuin"
keywords = ["llm", "tokens", "openai", "claude", "cli", "prompt", "tokuin"]
categories = ["command-line-utilities", "development-tools"]

[dependencies]
# CLI
clap = { version = "4.5", features = ["derive"] }

# Error handling
thiserror = "1.0"

# OpenAI tokenization
tiktoken-rs = { version = "0.6", optional = true }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Configuration
toml = "0.8"

# Async runtime (optional for future features)
tokio = { version = "1.0", features = ["full"], optional = true }

# File watching
notify = { version = "5.0", optional = true }

# Markdown parsing
pulldown-cmark = { version = "0.9", optional = true }

# SentencePiece for Gemini and other models (optional, requires CMake)
# Note: Gemini works without this using approximation, but for exact counts install CMake:
#   macOS: brew install cmake
#   Linux: apt-get install cmake (or yum install cmake)
#   Windows: Download from https://cmake.org/download/
sentencepiece = { version = "0.12", optional = true }

# HTTP client for load testing
reqwest = { version = "0.12", default-features = false, features = ["json", "stream", "rustls-tls"], optional = true }
async-trait = { version = "0.1", optional = true }

# OpenSSL with vendored feature for cross-compilation
openssl = { version = "0.10", features = ["vendored"], optional = true }

# Progress bars for load testing
indicatif = { version = "0.17", optional = true }

# Latency histograms for metrics
hdrhistogram = { version = "7.5", optional = true }

# YAML parsing for prompt files
serde_yaml = { version = "0.9", optional = true }

# Random number generation for think time
fastrand = { version = "2.0", optional = true }

# Directory traversal for prompt library analysis
walkdir = "2.4"

# String similarity for compression
strsim = { version = "0.11", optional = true }

# Regex for Hieratic parsing
regex = { version = "1.10", optional = true }

# Time for context library timestamps
chrono = { version = "0.4", optional = true, features = ["serde"] }

# UUID for anchor IDs
uuid = { version = "1.7", optional = true, features = ["v4", "serde"] }

# Embedding models for semantic scoring (optional)
# ONNX Runtime for embedding model inference
ort = { version = "2.0.0-rc.10", optional = true }
# Array operations for ONNX tensors
ndarray = { version = "0.15", optional = true }
# CPU detection for ONNX thread configuration
num_cpus = { version = "1.16", optional = true }

# Tokenizer for embeddings
tokenizers = { version = "0.15", optional = true }

# HuggingFace Hub for model downloading
hf-hub = { version = "0.3", optional = true }

# Directory utilities for model caching
dirs = { version = "5.0", optional = true }

[features]
default = ["openai"]
openai = ["tiktoken-rs"]
watch = ["notify", "tokio"]
markdown = ["pulldown-cmark"]
gemini = []  # Gemini works without sentencepiece (uses approximation)
# For exact Gemini tokenization, enable sentencepiece: gemini-sentencepiece = ["sentencepiece"]
gemini-sentencepiece = ["sentencepiece"]
load-test = ["tokio", "reqwest", "async-trait", "indicatif", "hdrhistogram", "serde_yaml", "fastrand", "openssl"]
compression = ["strsim", "regex", "chrono", "uuid"]
compression-embeddings = ["compression", "tokenizers", "hf-hub", "dirs", "ort", "ndarray", "num_cpus", "openssl"]
all = ["openai", "watch", "markdown", "gemini", "load-test", "compression", "compression-embeddings"]

[dev-dependencies]
# Testing
tempfile = "3.8"
httpmock = "0.7"

[lib]
name = "tokuin"
path = "src/lib.rs"

[[bin]]
name = "tokuin"
path = "src/main.rs"
